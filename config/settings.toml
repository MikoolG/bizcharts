# BizCharts Configuration
# Copy to settings.toml and fill in your values

[scraper]
# Polling interval in seconds (minimum 60 recommended)
poll_interval_seconds = 60

# 4chan API rate limit (DO NOT exceed 1)
rate_limit_per_second = 1

# Minimum seconds between thread updates
thread_update_interval = 10

# Board to scrape
board = "biz"

# Download thumbnails for image posts
download_thumbnails = true

# Full image download (more storage, better analysis)
download_full_images = false

[database]
# SQLite database for raw posts
sqlite_path = "data/posts.db"

# DuckDB database for analytics
duckdb_path = "data/analytics.duckdb"

# Image storage directory
images_dir = "data/images"

[sentiment]
# Minimum confidence to include in aggregations
confidence_threshold = 0.5

# Use Claude for posts below this confidence
vader_confidence_threshold = 0.6

# Send ambiguous posts to Claude for analysis
use_claude_for_ambiguous = true

# Sentiment score thresholds
bullish_threshold = 0.05
bearish_threshold = -0.05

# Greentext confidence multiplier (reduces confidence for ironic content)
greentext_confidence_factor = 0.6

[sentiment.fusion]
# Weight for text sentiment in fusion
text_weight = 0.6

# Weight for image sentiment in fusion
image_weight = 0.4

[claude]
# Claude model to use
model = "claude-3-5-haiku-20241022"

# Use Batch API for bulk processing (50% cost reduction)
use_batch_api = true

# Enable prompt caching (90% reduction on cached reads)
enable_prompt_caching = true

# Maximum tokens for response
max_tokens = 200

# Batch size for Batch API
batch_size = 1000

# Timeout for batch completion (seconds)
batch_timeout = 3600

[image_analysis]
# Use CLIP for zero-shot classification
use_clip = true

# CLIP model name
clip_model = "openai/clip-vit-base-patch32"

# Use YOLOv8 for Wojak/Pepe detection
use_yolo = true

# YOLOv8 model path (after training)
yolo_model_path = "models/wojak_detector.pt"

# Use PaddleOCR for meme text extraction
use_ocr = true

# OCR confidence threshold
ocr_confidence_threshold = 0.5

[aggregation]
# Time bucket sizes to compute
bucket_sizes = ["minute", "hour", "day", "week"]

# How often to run aggregation (seconds)
aggregation_interval = 300

# Keep raw sentiment scores for this many days
raw_retention_days = 30

# Keep aggregated data for this many days
aggregated_retention_days = 365

[fear_greed]
# Component weights for Fear/Greed index calculation
sentiment_weight = 0.40
ratio_weight = 0.30
volume_weight = 0.20
price_correlation_weight = 0.10

# Use price correlation in calculation (requires CoinGecko data)
include_price_correlation = true

[price_data]
# Coins to track for correlation (used by all sources)
tracked_coins = ["bitcoin", "ethereum", "solana"]

# How often to fetch price data (seconds)
# With 3-source rotation: each source gets 1/3 of calls
# 300s interval = 12 fetches/hour, 4 per source = 2,880/month each âœ“
price_fetch_interval = 300

# Enable multi-source rotation for rate limit management
enable_rotation = true

# Source rotation order (cycles through on each fetch)
rotation_order = ["coingecko", "binance", "coinmarketcap"]

# Fallback: if current source fails, try next in rotation
fallback_on_error = true

# Price deviation threshold (%) to flag potential data issues
# If sources differ by more than this, log a warning
price_deviation_threshold = 2.0

[price_data.sources.coingecko]
# CoinGecko - Primary source (aggregated from 900+ exchanges)
# Free Demo tier: 30 calls/min, 10,000 calls/month
enabled = true
api_url = "https://api.coingecko.com/api/v3"
rate_limit_per_minute = 30
monthly_limit = 10000
# Maps our coin names to CoinGecko IDs
coin_ids = { bitcoin = "bitcoin", ethereum = "ethereum", solana = "solana" }

[price_data.sources.binance]
# Binance - Real-time exchange prices (no rate limits on public data)
# Direct from world's largest exchange by volume
enabled = true
api_url = "https://data-api.binance.vision/api/v3"
rate_limit_per_minute = 1200
monthly_limit = 0  # 0 = unlimited
# Maps our coin names to Binance trading pairs (vs USDT)
symbols = { bitcoin = "BTCUSDT", ethereum = "ETHUSDT", solana = "SOLUSDT" }

[price_data.sources.coinmarketcap]
# CoinMarketCap - Aggregated data (requires free API key)
# Free tier: 10,000 calls/month
enabled = true
api_url = "https://pro-api.coinmarketcap.com/v1"
api_key_env = "COINMARKETCAP_API_KEY"  # Set this env var for CMC
rate_limit_per_minute = 30
monthly_limit = 10000
# Maps our coin names to CMC slugs
slugs = { bitcoin = "bitcoin", ethereum = "ethereum", solana = "solana" }

[grafana]
# Grafana datasource name for DuckDB
datasource_name = "BizCharts DuckDB"

# Dashboard refresh interval
refresh_interval = "1m"

[logging]
# Log level: DEBUG, INFO, WARNING, ERROR
level = "INFO"

# Log file path
file = "logs/bizcharts.log"

# Rotate logs when they reach this size (MB)
max_size_mb = 100

# Keep this many rotated log files
backup_count = 5

[alerts]
# Enable sentiment shift alerts
enabled = false

# Alert if Fear/Greed index changes by this much in 1 hour
shift_threshold = 15

# Webhook URL for alerts (Discord, Slack, etc.)
# webhook_url = "https://hooks.slack.com/..."

# Alert cooldown (seconds) - don't spam alerts
cooldown_seconds = 3600

# ============================================================================
# ML TRAINING CONFIGURATION
# ============================================================================

[ml.models]
# Paths to trained model directories
setfit_path = "python-ml/models/setfit"
cryptobert_path = "python-ml/models/cryptobert"
llava_path = "python-ml/models/llava"

# Model configurations
setfit_base_model = "sentence-transformers/paraphrase-mpnet-base-v2"
cryptobert_base_model = "ElKulako/cryptobert"
llava_base_model = "llava-hf/llava-1.5-7b-hf"

[ml.training]
# SetFit training parameters
setfit_batch_size = 16
setfit_epochs = 4
setfit_iterations = 20
setfit_test_size = 0.2

# CryptoBERT LoRA parameters
cryptobert_lora_r = 16
cryptobert_lora_alpha = 32
cryptobert_lora_dropout = 0.1

# LLaVA QLoRA parameters
llava_load_in_4bit = true
llava_max_new_tokens = 150

[ml.ensemble]
# Enable ensemble mode (requires trained models)
enabled = false

# Ensemble weights: VADER + ML models
vader_weight = 0.3
ml_weight = 0.7

[ml.fusion]
# Multi-modal fusion weights
text_weight = 0.5
image_weight = 0.3
ocr_weight = 0.1
context_weight = 0.1

# Sarcasm detection threshold
sarcasm_threshold = 0.7

# Apply sentiment inversion when sarcasm detected
apply_sarcasm_inversion = true

[ml.active_learning]
# Hybrid acquisition parameters
uncertainty_weight = 0.6
diversity_clusters = 50
selection_pool_size = 1000

# Labeling targets
target_labels = 500
labels_per_session = 50

[ml.continual]
# Experience replay buffer size
replay_buffer_size = 2000

# Replay ratio during retraining (30% old, 70% new)
replay_ratio = 0.3

# ADWIN drift detection sensitivity
drift_delta = 0.002

# Vocabulary fragmentation threshold for new term detection
fragmentation_threshold = 3

# Retraining schedule
weekly_retrain = true
weekly_retrain_day = "sunday"
monthly_evaluation = true

[ml.pseudo_labeling]
# FixMatch threshold for pseudo-labels
confidence_threshold = 0.95

# Lower threshold after model improves
improved_threshold = 0.90

# Maximum pseudo-labels per batch
max_pseudo_per_batch = 500

[ml.inference]
# Use ONNX Runtime for CPU inference
use_onnx = false

# Batch size for inference
batch_size = 32

# Device for inference ('cuda', 'cpu', 'auto')
device = "auto"
